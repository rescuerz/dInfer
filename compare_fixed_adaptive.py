#!/usr/bin/env python3
"""
dInfer å›ºå®švsè‡ªé€‚åº”ä¸“å®¶æ¿€æ´»æ¯”è¾ƒè„šæœ¬

æ¯”è¾ƒå›ºå®šä¸“å®¶æ•°å’ŒåŠ¨æ€ä¸“å®¶æ¿€æ´»çš„æ€§èƒ½å·®å¼‚ã€‚
"""

import os
import time
import argparse
import torch
from transformers import AutoTokenizer, AutoConfig
from vllm import distributed
from vllm.config import ParallelConfig, VllmConfig, set_current_vllm_config

from dinfer.model import FusedOlmoeForCausalLM
from dinfer import BlockIteratorFactory, KVCacheFactory
from dinfer import ThresholdParallelDecoder, BlockWiseDiffusionLLM, FixedParallelDecoder


def compare_fixed_vs_adaptive(model, tokenizer, input_ids, args):
    """æ¯”è¾ƒå›ºå®šä¸“å®¶æ¿€æ´»å’Œè‡ªé€‚åº”ä¸“å®¶æ¿€æ´»çš„å·®å¼‚

    Args:
        model: FusedOlmoeForCausalLM æ¨¡å‹
        tokenizer: tokenizer
        input_ids: è¾“å…¥ token idsï¼Œshape [1, seq_len]
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    print("\n" + "="*80)
    print("Comparison Mode: Fixed vs Adaptive Expert Activation")
    print("="*80)

    # 1. è¿è¡Œå›ºå®šä¸“å®¶æ¿€æ´»
    print("\nğŸ”¹ Running FIXED expert activation...")
    print("-"*80)

    # åˆ›å»ºè§£ç å™¨å’Œæ‰©æ•£LLMï¼ˆå›ºå®šæ¨¡å¼ï¼‰
    decoder_fixed = ThresholdParallelDecoder(
        temperature=0.0,
        threshold=args.threshold,
        mask_id=args.mask_id,
        eos_id=args.eos_id
    )
    # decoder_fixed = FixedParallelDecoder(
    #     temperature=0.0,
    #     steps=args.steps,
    #     mask_id=args.mask_id
    # )

    dllm_fixed = BlockWiseDiffusionLLM(
        model=model,
        decoder=decoder_fixed,
        iterator_factory=BlockIteratorFactory(True),
        early_stop=False,
        cache_factory=KVCacheFactory('dual'),
        enable_adaptive_moe=False  # å›ºå®šæ¨¡å¼
    )

    start_time_fixed = time.time()
    res_fixed = dllm_fixed.generate(
        input_ids,
        gen_length=args.gen_length,
        block_length=args.block_length
    )
    end_time_fixed = time.time()
    time_fixed = end_time_fixed - start_time_fixed
    nfe_fixed = dllm_fixed.num_forwards

    tokens_fixed = res_fixed[0, input_ids.shape[1]:]
    text_fixed = tokenizer.decode(tokens_fixed, skip_special_tokens=True)
    print(f"\nğŸ”¸ Generated Text (FIXED):")
    print("-" * 30)
    print(f"   {text_fixed}")
    print("-" * 30)

    # æ¸…ç†å›ºå®šæ¨¡å¼çš„çŠ¶æ€ï¼Œé¿å…å½±å“è‡ªé€‚åº”æ¨¡å¼
    del dllm_fixed, decoder_fixed
    torch.cuda.empty_cache()

    # 2. è¿è¡Œè‡ªé€‚åº”ä¸“å®¶æ¿€æ´»
    print(f"\nğŸ”¹ Running ADAPTIVE expert activation (initial={args.initial_num_experts}, max={args.max_num_experts})...")
    print("-"*80)

    # åˆ›å»ºè§£ç å™¨å’Œæ‰©æ•£LLMï¼ˆè‡ªé€‚åº”æ¨¡å¼ï¼‰
    decoder_adaptive = ThresholdParallelDecoder(
        temperature=0.0,
        threshold=args.threshold,
        mask_id=args.mask_id,
        eos_id=args.eos_id
    )
    # decoder_adaptive = FixedParallelDecoder(
    #     temperature=0.0,
    #     steps=args.steps,
    #     mask_id=args.mask_id
    # )

    dllm_adaptive = BlockWiseDiffusionLLM(
        model=model,
        decoder=decoder_adaptive,
        iterator_factory=BlockIteratorFactory(True),
        early_stop=False,
        cache_factory=KVCacheFactory('dual'),
        enable_adaptive_moe=True,  # å¯ç”¨è‡ªé€‚åº”
        growth_strategy=args.growth_strategy,
        max_num_experts=args.max_num_experts,
        initial_num_experts=args.initial_num_experts,
        update_interval=args.update_interval,
        verbose=args.verbose  # ä¼ é€’ verbose å‚æ•°
    )

    start_time_adaptive = time.time()
    res_adaptive = dllm_adaptive.generate(
        input_ids,
        gen_length=args.gen_length,
        block_length=args.block_length
    )
    end_time_adaptive = time.time()
    time_adaptive = end_time_adaptive - start_time_adaptive
    nfe_adaptive = dllm_adaptive.num_forwards

        
    tokens_adaptive = res_adaptive[0, input_ids.shape[1]:]
    text_adaptive = tokenizer.decode(tokens_adaptive, skip_special_tokens=True)
    print(f"\nGenerated Text (ADAPTIVE):")
    print("-" * 30)
    print(f"   {text_adaptive}")
    print("-" * 30)
    # 3. è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    num_tokens = args.gen_length
    tps_fixed = num_tokens / time_fixed if time_fixed > 0 else 0
    tps_adaptive = num_tokens / time_adaptive if time_adaptive > 0 else 0

    speedup = time_fixed / time_adaptive if time_adaptive > 0 else 0


    print(f"\nPerformance Metrics:")
    print(f"\n  FIXED Mode:")
    print(f"     Tokens generated:  {num_tokens}")
    print(f"     Time taken:        {time_fixed:.2f}s")
    print(f"     NFE:               {nfe_fixed}")
    print(f"     TPS:               {tps_fixed:.2f} tokens/sec")

    print(f"\n  ADAPTIVE Mode (initial={args.initial_num_experts}):")
    print(f"     Tokens generated:  {num_tokens}")
    print(f"     Time taken:        {time_adaptive:.2f}s")
    print(f"     NFE:               {nfe_adaptive}")
    print(f"     TPS:               {tps_adaptive:.2f} tokens/sec")

    print(f"\n  Speedup:           {speedup:.2f}x")
    print(f"  Time reduction:    {(1 - time_adaptive/time_fixed)*100:.1f}%")

    # 4. æ‰“å°è¯¦ç»†æ—¥å¿—ï¼ˆå¦‚æœå¯ç”¨äº† verboseï¼‰
    if args.verbose and hasattr(dllm_adaptive.diff_iteration, 'step_logs'):
        print("\n" + "="*80)
        print("ğŸ“‹ Detailed Step-by-Step Logs (ADAPTIVE Mode)")
        print("="*80)

        step_logs = dllm_adaptive.diff_iteration.step_logs
        for i, log in enumerate(step_logs):
            # if i % 8 == 0:
            print("="*40)
            print(f"Step {log['step']}:")
            print("="*40) 
            # æ‰“å°ä¸“å®¶é…ç½®
            if log['num_experts_per_tok_global'] is not None:
                print(f"\n  Expert Configuration in Block [{log['block_range'][0]}:{log['block_range'][1]}]:")
                # æ‰“å°å½“å‰å—çš„ä¸“å®¶é…ç½®
                if log['experts_per_tok_in_block'] is not None:
                    experts_in_block = log['experts_per_tok_in_block'][0]  # [block_size]
                    print(f"    num_experts_per_tok (current block): {experts_in_block.tolist()}")
                global_experts = log['num_experts_per_tok_global'][0]  # [total_len]
                prompt_len = input_ids.shape[1]
                
                # æ˜¾ç¤ºpromptå’Œç”Ÿæˆéƒ¨åˆ†çš„ä¸“å®¶é…ç½®åˆ†å¸ƒ
                prompt_experts = global_experts[:prompt_len]
                gen_experts = global_experts[prompt_len:]

                # print(f"    Prompt part experts: {prompt_experts.tolist()}")
                print(f"Generation part experts: {gen_experts.tolist()}")

            # æ‰“å°å½“å‰åºåˆ—çŠ¶æ€ï¼ˆåªæ˜¾ç¤ºç”Ÿæˆéƒ¨åˆ†ï¼‰
            # if log['sequence_snapshot'] is not None:
            #     seq = log['sequence_snapshot'][0]  # [total_len]
            #     prompt_len = input_ids.shape[1]
            #     gen_part = seq[prompt_len:]  # åªæ˜¾ç¤ºç”Ÿæˆéƒ¨åˆ†

            #     # ç»Ÿè®¡MASK tokenæ•°é‡
            #     num_mask = (gen_part == args.mask_id).sum().item()
            #     num_decoded = len(gen_part) - num_mask

            #     print(f"\nSequence Status (Generated part: {num_decoded}/{len(gen_part)} decoded):")
            #     decoded_tokens = [tokenizer.decode([tok], skip_special_tokens=True) if tok != args.mask_id else '[MASK]'
            #                     for tok in gen_part[:].tolist()] 
            #     decoded_str = ' '.join(decoded_tokens)
            #     print(f"    {decoded_str}")
                


def main():
    parser = argparse.ArgumentParser(
        description='Compare Fixed vs Adaptive Expert Activation in dInfer'
    )

    # Model arguments
    parser.add_argument('--model_path', type=str, required=True,
                        help='Path to the fused MoE model')
    parser.add_argument('--prompt', type=str,
                        default="Lily can run 12 kilometers per hour for 4 hours. "
                                "After that, she can run 6 kilometers per hour. "
                                "How many kilometers can she run in 8 hours?",
                        help='Input prompt for text generation')

    # Generation arguments
    parser.add_argument('--gen_length', type=int, default=512,
                        help='Length of generated sequence (default: 512)')
    parser.add_argument('--block_length', type=int, default=64,
                        help='Block length for generation (default: 64)')
    parser.add_argument('--steps', type=int, default=128,
                        help='Number of diffusion steps (default: 128)')
    parser.add_argument('--threshold', type=float, default=0.9,
                        help='Confidence threshold for decoder (default: 0.9)')

    # Special token IDs
    parser.add_argument('--mask_id', type=int, default=156895,
                        help='Mask token ID (default: 156895)')
    parser.add_argument('--eos_id', type=int, default=156892,
                        help='EOS token ID (default: 156892)')

    # Adaptive expert configuration
    parser.add_argument('--growth_strategy', type=str, default='linear',
                        choices=['linear', 'exponential'],
                        help='Expert growth strategy (default: linear)')
    parser.add_argument('--initial_num_experts', type=int, default=1,
                        help='Initial number of experts for MASK tokens (default: 1)')
    parser.add_argument('--max_num_experts', type=int, default=8,
                        help='Maximum number of experts per token (default: 8)')
    parser.add_argument('--update_interval', type=int, default=8,
                        help='Update expert count every N steps (default: 8)')

    # Output control
    parser.add_argument('--show_text', action='store_true',
                        help='Show generated text (default: False)')
    parser.add_argument('--verbose', action='store_true',
                        help='Enable detailed step-by-step logging for adaptive mode (default: False)')

    # Device arguments
    parser.add_argument('--device', type=str, default='cuda:0',
                        help='Device to run on (default: cuda:0)')

    args = parser.parse_args()

    # ========== æ­¥éª¤1: ç¯å¢ƒåˆå§‹åŒ– ==========
    print("\n" + "="*80)
    print("dInfer Fixed vs Adaptive Expert Activation Comparison")
    print("="*80)

    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
    os.environ['CUDA_VISIBLE_DEVICES'] = args.device.split(':')[-1]
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12346'

    device = torch.device('cuda:0')

    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    distributed.init_distributed_environment(1, 0, 'env://', 0, 'nccl')
    distributed.initialize_model_parallel(1, backend='nccl')

    # ========== æ­¥éª¤2: åŠ è½½æ¨¡å‹ ==========
    print(f"\nLoading model from: {args.model_path}")

    parallel_config = ParallelConfig(enable_expert_parallel=True)
    with set_current_vllm_config(VllmConfig(parallel_config=parallel_config)):
        model_config = AutoConfig.from_pretrained(args.model_path, trust_remote_code=True)
        model = FusedOlmoeForCausalLM(config=model_config).eval()
        model.load_weights(args.model_path, torch_dtype=torch.bfloat16)
        model = model.to(device)

    tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True)

    # ========== æ­¥éª¤3: å‡†å¤‡è¾“å…¥ ==========
    print(f"\nPrompt: {args.prompt}")

    m = [{"role": "user", "content": args.prompt}]
    prompt_text = tokenizer.apply_chat_template(m, add_generation_prompt=True, tokenize=False)
    input_ids = tokenizer(prompt_text)['input_ids']
    input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)

    print(f"\nConfiguration:")
    print(f"   Generation length:    {args.gen_length}")
    print(f"   Block length:         {args.block_length}")
    print(f"   Diffusion steps:      {args.steps}")
    print(f"   Threshold:            {args.threshold}")
    print(f"   Growth strategy:      {args.growth_strategy}")
    print(f"   Initial experts:      {args.initial_num_experts}")
    print(f"   Max experts:          {args.max_num_experts}")
    print(f"   Update interval:      {args.update_interval}")

    # ========== æ­¥éª¤4: è¿è¡Œæ¯”è¾ƒ ==========
    compare_fixed_vs_adaptive(model, tokenizer, input_ids, args)


if __name__ == "__main__":
    main()
